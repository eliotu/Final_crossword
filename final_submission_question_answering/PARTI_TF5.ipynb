{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b0476-2f61-40ae-99b1-3e25f82fd03c",
   "metadata": {},
   "source": [
    "#                                                     TF5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e456d56-46d0-487c-b6cf-70153b1d3952",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1ac542-45c6-4003-acc3-1e744315099c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sources: 433034\n",
      "train_targets: 433034\n",
      "val_sources: 72304\n",
      "val_targets: 72304\n",
      "test_sources: 72940\n",
      "test_targets: 72940\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "def load_data(source_file, target_file):\n",
    "    with open(source_file, 'r') as f:\n",
    "        sources = f.read().splitlines()\n",
    "    with open(target_file, 'r') as f:\n",
    "        targets = f.read().splitlines()\n",
    "    return sources, targets\n",
    "\n",
    "train_sources, train_targets = load_data('dataset/train.source', 'dataset/train.target')\n",
    "val_sources, val_targets = load_data('dataset/val.source', 'dataset/val.target')\n",
    "test_sources, test_targets = load_data('dataset/test.source', 'dataset/test.target')\n",
    "\n",
    "\n",
    "print(\"train_sources:\", len(train_sources))\n",
    "print(\"train_targets:\", len(train_targets))\n",
    "print(\"val_sources:\", len(val_sources))\n",
    "print(\"val_targets:\", len(val_targets))\n",
    "print(\"test_sources:\", len(test_sources))\n",
    "print(\"test_targets:\", len(test_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aab2e8-e46f-4dab-baf9-78b9ce84b8bc",
   "metadata": {},
   "source": [
    "## Initialization of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8a1d5-fcd4-4013-afde-67491a6db999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeab4e3-fa8c-461f-b11e-afde809cd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class T5Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, targets, max_length=512, max_target_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.max_length = max_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_encoded = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target_encoded = self.tokenizer(\n",
    "            self.targets[idx],\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_encoded['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': source_encoded['attention_mask'].squeeze(),\n",
    "            'labels': target_encoded['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "# Apply the dataset class\n",
    "train_dataset = T5Dataset(tokenizer, train_sources, train_targets)\n",
    "val_dataset = T5Dataset(tokenizer, val_sources, val_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5dc32-457b-432d-8d7d-c5a1e0d8c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859ccf0-9f2f-465d-9434-b9efec953137",
   "metadata": {},
   "source": [
    "# Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc478fd2-e255-4c98-a16a-400a5090f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"results/checkpoint-162000\"\n",
    "\n",
    "# Load the model from a checkpoint\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d7f27-b328-4bfb-ae7a-a713e6d483ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def predict_words(clue, model, k, num_predictions=500):\n",
    "    # Prepare the model input\n",
    "    input_ids = tokenizer.encode(clue, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            num_return_sequences=num_predictions, \n",
    "            max_length=k + 1,  # Add 1 to accommodate for special tokens\n",
    "            num_beams=num_predictions,\n",
    "            early_stopping=False  # Stops generation when all beam hypotheses reached the EOS token\n",
    "        )\n",
    "    \n",
    "    # Decode the predictions and filter by length\n",
    "    predictions = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in outputs]\n",
    "    filtered_predictions = [word for word in predictions if len(word) == k]\n",
    "    \n",
    "    return filtered_predictions\n",
    "\n",
    "# Example usage:\n",
    "clue = \"crossword: city in portugal is:\"\n",
    "word_length = 6\n",
    "predictions = predict_words(clue,model, word_length)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225ed75-2968-4406-9c07-736ec9839979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
