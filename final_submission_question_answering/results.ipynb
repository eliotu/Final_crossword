{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611a6983-b633-451b-b3d8-01e830c087dd",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c830a0d-67a5-4a7e-83fa-63aa6b1cd3c6",
   "metadata": {},
   "source": [
    "THIS IS A JUPYTER NOTEBOOK TO COMPARE OUR MODELS WITH OUR TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23138abf-98f6-443d-bc90-901bec802551",
   "metadata": {},
   "source": [
    "LOADING TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5522b80e-da0a-4a5a-80a9-1ca7808f0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "def load_data(source_file, target_file):\n",
    "    with open(source_file, 'r') as f:\n",
    "        sources = f.read().splitlines()\n",
    "    with open(target_file, 'r') as f:\n",
    "        targets = f.read().splitlines()\n",
    "    return sources, targets\n",
    "\n",
    "\n",
    "test_sources, test_targets = load_data('dataset/filtered_test.source', 'dataset/filtered_test.target')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8014a0-3f7d-45b0-89ec-9290560f59c6",
   "metadata": {},
   "source": [
    "#### Prediction with LESK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff605960-be22-402d-a30a-07a55fcc25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_lesk(sent, k):\n",
    "    text_words = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp_words = list(set(text_words) - set(stop_words))\n",
    "    if len(temp_words) != 0:\n",
    "        text_words = temp_words\n",
    "    word_synsets = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        word_synsets.extend(wn.synsets(val))\n",
    "\n",
    "    output_lesk = {}\n",
    "    context_words = []\n",
    "\n",
    "    for i, val in enumerate(word_synsets):\n",
    "        words = word_tokenize(val.definition())\n",
    "        context_words.extend(words)\n",
    "\n",
    "    context_words = list(set(context_words) - set(stop_words))\n",
    "    \n",
    "    if len(text_words) == 0:\n",
    "        text_words = [1]  # Prevent division by zero by using a dummy count\n",
    "    if len(context_words) == 0:\n",
    "        context_words = [1]  # Prevent division by zero by using a dummy count\n",
    "\n",
    "    for word in list(wn.all_lemma_names(lang='eng')):\n",
    "        if len(word) == k and word.isalpha():\n",
    "            lesk = 0\n",
    "            for synset in list(wn.synsets(word)):\n",
    "                def_words = word_tokenize(synset.definition())\n",
    "                def_words = list(set(def_words) - set(stop_words))\n",
    "                if len(text_words) > 0:\n",
    "                    temp1 = len(set(text_words).intersection(def_words)) / len(text_words)\n",
    "                else:\n",
    "                    temp1 = 0\n",
    "                if len(context_words) > 0:\n",
    "                    temp2 = len(set(context_words).intersection(def_words)) / len(context_words)\n",
    "                else:\n",
    "                    temp2 = 0\n",
    "                t = 0.75 * temp1 + 0.25 * temp2\n",
    "                lesk = max(t, lesk)\n",
    "            if lesk > 0:\n",
    "                output_lesk[word] = lesk\n",
    "\n",
    "    rank = list(sorted(output_lesk, key=output_lesk.__getitem__, reverse=True))\n",
    "    return rank[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "608d1482-bc03-41a0-a353-f058b9df050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lesk(test_sources, test_targets):\n",
    "    correct_count = 0\n",
    "    total = len(test_sources)\n",
    "    results = []\n",
    "\n",
    "    for source, target in zip(test_sources, test_targets):\n",
    "        predictions = modified_lesk(source, len(target))\n",
    "        if target in predictions:\n",
    "            results.append(\"yes\")\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            results.append(\"no\")\n",
    "    \n",
    "    # Calculate the probability of correct predictions\n",
    "    probability_yes = correct_count / total\n",
    "\n",
    "    return results, probability_yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d4c2f51-a57e-4a55-9be2-6ee5a22444ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'no',\n",
       "  ...],\n",
       " 0.15716810617934437)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_lesk(test_sources, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb07b3-f010-4633-8eef-2eeda179fd6c",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b4fa1-1272-4f06-af3e-d6a51da9c897",
   "metadata": {},
   "source": [
    "### TF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a91fd93-73a6-4530-8510-0954e857ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "def evaluate(test_sources, test_targets,  tokenizer, model,num_predictions):\n",
    "    correct_count = 0\n",
    "    total = len(test_sources)\n",
    "    results = []\n",
    "    i=0\n",
    "    for source, target in zip(test_sources, test_targets):\n",
    "        if(i%1000==0):\n",
    "            print(i)\n",
    "            print(correct_count)\n",
    "        i+=1\n",
    "        # Assume target is the correct word and its length is the desired prediction length\n",
    "        predictions = predict_words3(source,tokenizer, model, len(target), num_predictions)\n",
    "        \n",
    "        # Check if the correct target is in the predictions\n",
    "        if target in predictions:\n",
    "            results.append(\"yes\")\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            results.append(\"no\")\n",
    "    \n",
    "    # Calculate the probability of correct predictions\n",
    "    probability_yes = correct_count / total\n",
    "\n",
    "    return results, probability_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab25557-dbef-47ea-bd6d-d67f02ea3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words3(clue,tokenizer, model, k, num_predictions=500):\n",
    "    # Prepare the model input\n",
    "    input_ids = tokenizer.encode(clue, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            num_return_sequences=num_predictions, \n",
    "            max_length=k + 1,  # Add 1 to accommodate for special tokens\n",
    "            num_beams=num_predictions,\n",
    "            early_stopping=False  # Stops generation when all beam hypotheses reached the EOS token\n",
    "        )\n",
    "    \n",
    "    # Decode the predictions and filter by length\n",
    "    predictions = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in outputs]\n",
    "    filtered_predictions = [word for word in predictions if len(word) == k]\n",
    "    \n",
    "    return filtered_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8a410-8c07-4d30-aebf-bd5ebb5c15d0",
   "metadata": {},
   "source": [
    "### pre tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8bbfe-b503-455e-b584-d6ac6242de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57310f89-c249-497a-b177-e3bb65a02e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_sources,test_targets,tokenizer,model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c71cf-c0fb-4d49-a74c-5e0ca7c192e4",
   "metadata": {},
   "source": [
    "### fine tunned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba190d-9697-4a49-90b2-5bce45d443c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('results/checkpoint-162000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc372c96-b3bf-4c8c-b698-37f9283a47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_sources,test_targets,tokenizer,model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39507b-f462-4229-9aa0-6ad0391f1032",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370941d-5e48-4c36-aabb-bc469007167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "def predict_multiple_answers_bert(model, tokenizer, text, num_answers=3, wanted_size=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize input text and add special tokens\n",
    "    encoded_input = tokenizer.encode_plus(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
    "    input_ids = encoded_input['input_ids'].to(device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "    # Identify the position of the masked token\n",
    "    mask_position = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Softmax the result to get probabilities\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    mask_word_probs = softmax(predictions[0, mask_position])\n",
    "\n",
    "    # Get the top 'num_answers' token predictions for the masked position\n",
    "    top_tokens_probs, top_tokens_ids = torch.topk(mask_word_probs, num_answers * 10, dim=1)  # Increase factor to find enough valid candidates\n",
    "    answers = []\n",
    "\n",
    "    for idx in top_tokens_ids[0]:\n",
    "        token = tokenizer.decode([idx]).strip()\n",
    "        if len(token) == wanted_size:\n",
    "            answers.append(token)\n",
    "        if len(answers) >= num_answers:\n",
    "            break\n",
    "\n",
    "    # If not enough answers of wanted_size are found, fill the rest with placeholders or a message\n",
    "    while len(answers) < num_answers:\n",
    "        answers.append(f\"No more {wanted_size}-char words\")\n",
    "\n",
    "    return answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1a6b9-880c-43da-938c-aa1468e20b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "def evaluate(test_sources, test_targets,  tokenizer, model,num_predictions):\n",
    "    correct_count = 0\n",
    "    total = len(test_sources)\n",
    "    results = []\n",
    "    i=0\n",
    "    for source, target in zip(test_sources, test_targets):\n",
    "        if(i%1000==0):\n",
    "            print(i)\n",
    "            print(correct_count)\n",
    "        i+=1\n",
    "        # Assume target is the correct word and its length is the desired prediction length\n",
    "        predictions = predict_multiple_answers_bert(model,tokenizer, source, num_answers=num_predictions,wanted_size=len(target))\n",
    "        \n",
    "        # Check if the correct target is in the predictions\n",
    "        if target in predictions:\n",
    "            results.append(\"yes\")\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            results.append(\"no\")\n",
    "    \n",
    "    # Calculate the probability of correct predictions\n",
    "    probability_yes = correct_count / total\n",
    "\n",
    "    return results, probability_yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e455c-341e-4cf0-babf-98d8776a13ea",
   "metadata": {},
   "source": [
    "### pre tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72dcf5-1a0c-4a88-aa7d-6666c05c4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1292a8e-dfcd-4f27-ad00-1f0629155e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_sources,test_targets,tokenizer,model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb310d-d9a7-4c24-969e-ae9280b827c9",
   "metadata": {},
   "source": [
    "### fine tunned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77010c44-bbf6-4e32-a473-c1a4bf8a6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('results/checkpoint-81000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8179f7-e32f-4b2e-81a4-1f27e96d83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_sources,test_targets,tokenizer,model,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
