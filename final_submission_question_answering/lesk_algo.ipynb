{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d4f0cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (4.66.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eliotullmo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eliotullmo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eliotullmo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831e503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f433a96b",
   "metadata": {},
   "source": [
    "Lets try to implement a simple algorithm based on this paper: key to crossword solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3878845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_lesk(sent, k):\n",
    "    text_words = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp_words = list(set(text_words) - set(stop_words))\n",
    "    if len(temp_words) != 0:\n",
    "        text_words = temp_words\n",
    "    word_synsets = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        word_synsets.extend(wn.synsets(val))\n",
    "\n",
    "    output_lesk = {}\n",
    "    context_words = []\n",
    "\n",
    "    for i, val in enumerate(word_synsets):\n",
    "        words = word_tokenize(val.definition())\n",
    "        context_words.extend(words)\n",
    "\n",
    "    context_words = list(set(context_words) - set(stop_words))\n",
    "    print(context_words)\n",
    "    print(\"done\")\n",
    "    for word in list(wn.all_lemma_names(lang='eng')):\n",
    "        if len(word) == k and word.isalpha():\n",
    "            lesk = 0\n",
    "            for synset in list(wn.synsets(word)):\n",
    "                def_words = word_tokenize(synset.definition())\n",
    "                def_words = list(set(def_words) - set(stop_words))\n",
    "                temp1 = set(text_words).intersection(def_words)\n",
    "                temp2 = set(context_words).intersection(def_words)\n",
    "                temp1 = len(temp1) / len(text_words)\n",
    "                temp2 = len(temp2) / len(context_words)\n",
    "                t = 0.75 * temp1 + 0.25 * temp2\n",
    "                lesk = max(t, lesk)\n",
    "            if lesk > 0:\n",
    "                output_lesk[word] = lesk\n",
    "\n",
    "    rank = list(sorted(output_lesk, key=output_lesk.__getitem__, reverse=True))\n",
    "    return rank[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5358011a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', 'colonized', 'France', 'cooking', 'indefinite', 'pertaining', 'language', 'make', 'cities', 'body', 'people', 'countries', 'outside', 'beans', 'public', 'Washington', 'sculptor', 'region', 'talk', 'capable', 'States', 'purpose', 'involving', 'utterance', 'delivering', 'live', 'usually', 'characteristic', 'boundary', 'area', 'Romance', 'special', 'audience', 'use', 'serving', 'single', 'exchange', 'address', 'occupied', '1850-1931', 'speech', ';', 'e.g', 'geographical', 'nation', 'express', 'United', 'culture', 'cut', 'politically', 'particular', 'territory', 'geography', 'Abraham', ',', 'intelligible', 'created', 'seated', 'spoken', 'thoughts', 'natural', 'Lincoln', 'D.C.', 'figure', 'speaking', 'marble', 'government', 'give', 'country', 'organized', 'Memorial', '(', 'lengthwise', 'preparation', 'distinguished', 'towns', 'sound']\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['brasil',\n",
       " 'brazil',\n",
       " 'estate',\n",
       " 'tweedy',\n",
       " 'arroba',\n",
       " 'cathay',\n",
       " 'colony',\n",
       " 'defect',\n",
       " 'desert',\n",
       " 'greece',\n",
       " 'nation',\n",
       " 'russia',\n",
       " 'thrace',\n",
       " 'soviet',\n",
       " 'tinpot',\n",
       " 'bharat',\n",
       " 'chorus',\n",
       " 'cyprus',\n",
       " 'export',\n",
       " 'fealty',\n",
       " 'import',\n",
       " 'manchu',\n",
       " 'source',\n",
       " 'sultan',\n",
       " 'tungus',\n",
       " 'altaic',\n",
       " 'folksy',\n",
       " 'native',\n",
       " 'rustic',\n",
       " 'amhara',\n",
       " 'barrio',\n",
       " 'belize',\n",
       " 'canaan',\n",
       " 'canton',\n",
       " 'consul',\n",
       " 'france',\n",
       " 'people',\n",
       " 'prague',\n",
       " 'salish',\n",
       " 'toltec',\n",
       " 'watusi',\n",
       " 'zurich',\n",
       " 'wander',\n",
       " 'abroad',\n",
       " 'regent',\n",
       " 'upland',\n",
       " 'mutely',\n",
       " 'armory',\n",
       " 'emigre',\n",
       " 'mbundu',\n",
       " 'muzzle',\n",
       " 'pathan',\n",
       " 'rapine',\n",
       " 'squire',\n",
       " 'tirana',\n",
       " 'tongue',\n",
       " 'townee',\n",
       " 'deport',\n",
       " 'french',\n",
       " 'county',\n",
       " 'romani',\n",
       " 'romany',\n",
       " 'bomber',\n",
       " 'busker',\n",
       " 'calder',\n",
       " 'disney',\n",
       " 'hoagie',\n",
       " 'keller',\n",
       " 'rhodes',\n",
       " 'talker',\n",
       " 'truman',\n",
       " 'varese',\n",
       " 'echoic',\n",
       " 'formal',\n",
       " 'magyar',\n",
       " 'nepali',\n",
       " 'sacred',\n",
       " 'access',\n",
       " 'albers',\n",
       " 'apache',\n",
       " 'arnold',\n",
       " 'bartok',\n",
       " 'benton',\n",
       " 'bihari',\n",
       " 'carson',\n",
       " 'carver',\n",
       " 'chavez',\n",
       " 'cooper',\n",
       " 'coyote',\n",
       " 'dubyuh',\n",
       " 'edison',\n",
       " 'employ',\n",
       " 'feifer',\n",
       " 'ferber',\n",
       " 'fixing',\n",
       " 'galton',\n",
       " 'girard',\n",
       " 'gospel',\n",
       " 'gracie',\n",
       " 'groove',\n",
       " 'harris',\n",
       " 'henson',\n",
       " 'hiccup',\n",
       " 'hoover',\n",
       " 'hughes',\n",
       " 'inchon',\n",
       " 'jargon',\n",
       " 'kaolin',\n",
       " 'mallon',\n",
       " 'markup',\n",
       " 'method',\n",
       " 'monroe',\n",
       " 'paring',\n",
       " 'patois',\n",
       " 'peirce',\n",
       " 'powell',\n",
       " 'rankin',\n",
       " 'rastas',\n",
       " 'salver',\n",
       " 'sanger',\n",
       " 'simile',\n",
       " 'spider',\n",
       " 'strike',\n",
       " 'stuart',\n",
       " 'tanguy',\n",
       " 'tarawa',\n",
       " 'taylor',\n",
       " 'turner',\n",
       " 'wilson',\n",
       " 'zanuck',\n",
       " 'active',\n",
       " 'ammino',\n",
       " 'beaten',\n",
       " 'beetle',\n",
       " 'broken',\n",
       " 'creole',\n",
       " 'dulled',\n",
       " 'finite',\n",
       " 'fitted',\n",
       " 'foster',\n",
       " 'german',\n",
       " 'hipped',\n",
       " 'limbed',\n",
       " 'limpid',\n",
       " 'living',\n",
       " 'medium',\n",
       " 'middle',\n",
       " 'norman',\n",
       " 'nursed',\n",
       " 'paying',\n",
       " 'prompt',\n",
       " 'ringed',\n",
       " 'scotch',\n",
       " 'shaken',\n",
       " 'spoken',\n",
       " 'staple',\n",
       " 'tropic',\n",
       " 'tufted',\n",
       " 'uremic',\n",
       " 'valent',\n",
       " 'wasted',\n",
       " 'yankee',\n",
       " 'yonder',\n",
       " 'zaftig',\n",
       " 'zoftig',\n",
       " 'acuity',\n",
       " 'aeolia',\n",
       " 'aeolis',\n",
       " 'ahpcrc',\n",
       " 'alarum',\n",
       " 'alcott',\n",
       " 'algren',\n",
       " 'amazon',\n",
       " 'aragon',\n",
       " 'arendt',\n",
       " 'arthur',\n",
       " 'asimov',\n",
       " 'autism',\n",
       " 'bailey',\n",
       " 'bangla',\n",
       " 'baraka',\n",
       " 'barber',\n",
       " 'barnum',\n",
       " 'barrie',\n",
       " 'baruch',\n",
       " 'bataan',\n",
       " 'beadle',\n",
       " 'bellow',\n",
       " 'berlin',\n",
       " 'bierce',\n",
       " 'bogart',\n",
       " 'bombay',\n",
       " 'bonney',\n",
       " 'breuer',\n",
       " 'brooks',\n",
       " 'browne',\n",
       " 'bunche',\n",
       " 'burger',\n",
       " 'bypass',\n",
       " 'cabell',\n",
       " 'caesar',\n",
       " 'cagney',\n",
       " 'callas',\n",
       " 'calvin',\n",
       " 'canopy',\n",
       " 'canute',\n",
       " 'capone',\n",
       " 'carter',\n",
       " 'casque',\n",
       " 'cather',\n",
       " 'cavity',\n",
       " 'chains',\n",
       " 'chopin',\n",
       " 'ciardi',\n",
       " 'clergy',\n",
       " 'crosby',\n",
       " 'crouse',\n",
       " 'custer',\n",
       " 'darrow',\n",
       " 'datril',\n",
       " 'didion',\n",
       " 'digram',\n",
       " 'dimash',\n",
       " 'domino',\n",
       " 'drogue',\n",
       " 'dulles',\n",
       " 'duncan',\n",
       " 'durant',\n",
       " 'ederle',\n",
       " 'edward',\n",
       " 'energy',\n",
       " 'epilog',\n",
       " 'erving',\n",
       " 'eskimo',\n",
       " 'europe',\n",
       " 'fantan',\n",
       " 'farmer',\n",
       " 'father',\n",
       " 'fields',\n",
       " 'flagon',\n",
       " 'franck',\n",
       " 'fuller',\n",
       " 'gammon',\n",
       " 'geisel',\n",
       " 'gesell',\n",
       " 'gibran',\n",
       " 'gibson',\n",
       " 'gilman',\n",
       " 'gilmer',\n",
       " 'glaser',\n",
       " 'gobble',\n",
       " 'godard',\n",
       " 'gorgas',\n",
       " 'gorgon',\n",
       " 'graham',\n",
       " 'greens',\n",
       " 'ground',\n",
       " 'groves',\n",
       " 'harbor',\n",
       " 'harlow',\n",
       " 'hassam',\n",
       " 'hearst',\n",
       " 'hearth',\n",
       " 'helium',\n",
       " 'heller',\n",
       " 'herman',\n",
       " 'holler',\n",
       " 'holloa',\n",
       " 'holmes',\n",
       " 'hooker',\n",
       " 'horney',\n",
       " 'hubble',\n",
       " 'huston',\n",
       " 'icecap',\n",
       " 'irving',\n",
       " 'iskcon',\n",
       " 'jacobs',\n",
       " 'jambon',\n",
       " 'jenner',\n",
       " 'jolson',\n",
       " 'joplin',\n",
       " 'joseph',\n",
       " 'kameez',\n",
       " 'keaton',\n",
       " 'kennan',\n",
       " 'keokuk',\n",
       " 'keynes',\n",
       " 'khanty',\n",
       " 'kinsey',\n",
       " 'kludge',\n",
       " 'kuiper',\n",
       " 'laffer',\n",
       " 'larium',\n",
       " 'laurel',\n",
       " 'lemmon',\n",
       " 'lerner',\n",
       " 'lescol',\n",
       " 'lesion',\n",
       " 'letter',\n",
       " 'levant',\n",
       " 'lexeme',\n",
       " 'liston',\n",
       " 'locker',\n",
       " 'london',\n",
       " 'lowell',\n",
       " 'lozier',\n",
       " 'lugosi',\n",
       " 'maffia',\n",
       " 'mailer',\n",
       " 'mallet',\n",
       " 'manioc',\n",
       " 'mantle',\n",
       " 'mantra',\n",
       " 'martin',\n",
       " 'matrix',\n",
       " 'mcgraw',\n",
       " 'mellon',\n",
       " 'merman',\n",
       " 'merton',\n",
       " 'millay',\n",
       " 'miller',\n",
       " 'minder',\n",
       " 'monody',\n",
       " 'morgan',\n",
       " 'morley',\n",
       " 'morris',\n",
       " 'morton',\n",
       " 'muffle',\n",
       " 'muller',\n",
       " 'mumbai',\n",
       " 'murrow',\n",
       " 'musial',\n",
       " 'napier',\n",
       " 'newman',\n",
       " 'nicker',\n",
       " 'nimitz',\n",
       " 'norris',\n",
       " 'oakley',\n",
       " 'oliver',\n",
       " 'oracle',\n",
       " 'ostyak',\n",
       " 'oswald',\n",
       " 'outfit',\n",
       " 'output',\n",
       " 'paiute',\n",
       " 'pajama',\n",
       " 'palmer',\n",
       " 'pander',\n",
       " 'parity',\n",
       " 'parker',\n",
       " 'pavise',\n",
       " 'person',\n",
       " 'phloem',\n",
       " 'pierce',\n",
       " 'pincus',\n",
       " 'piston',\n",
       " 'planet',\n",
       " 'plasma',\n",
       " 'porter',\n",
       " 'pyjama',\n",
       " 'rabies',\n",
       " 'ranier',\n",
       " 'ravage',\n",
       " 'reagan',\n",
       " 'reagin',\n",
       " 'regard',\n",
       " 'region',\n",
       " 'result',\n",
       " 'review',\n",
       " 'rhymer',\n",
       " 'robert',\n",
       " 'rogers',\n",
       " 'rothko',\n",
       " 'runyon',\n",
       " 'salwar',\n",
       " 'samite',\n",
       " 'savara',\n",
       " 'schulz',\n",
       " 'seaman',\n",
       " 'seeger',\n",
       " 'serkin',\n",
       " 'sermon',\n",
       " 'seward',\n",
       " 'sexton',\n",
       " 'shirer',\n",
       " 'singer',\n",
       " 'sodium',\n",
       " 'soiree',\n",
       " 'sontag',\n",
       " 'sperry',\n",
       " 'sphinx',\n",
       " 'stella',\n",
       " 'stream',\n",
       " 'streep',\n",
       " 'stroll',\n",
       " 'styron',\n",
       " 'sumner',\n",
       " 'sunday',\n",
       " 'switch',\n",
       " 'tampon',\n",
       " 'tanoan',\n",
       " 'tappan',\n",
       " 'teller',\n",
       " 'telugu',\n",
       " 'tempra',\n",
       " 'terror',\n",
       " 'thomas',\n",
       " 'thorpe',\n",
       " 'tickle',\n",
       " 'tilden',\n",
       " 'tipple',\n",
       " 'tocsin',\n",
       " 'toklas',\n",
       " 'townes',\n",
       " 'trivia',\n",
       " 'trumbo',\n",
       " 'tubman',\n",
       " 'tucker',\n",
       " 'tunnel',\n",
       " 'tunney',\n",
       " 'updike',\n",
       " 'upjohn',\n",
       " 'veblen',\n",
       " 'vinson',\n",
       " 'vodoun',\n",
       " 'volume',\n",
       " 'voodoo',\n",
       " 'walker',\n",
       " 'waller',\n",
       " 'warhol',\n",
       " 'warner',\n",
       " 'warren',\n",
       " 'waters',\n",
       " 'watson',\n",
       " 'welles',\n",
       " 'werfel',\n",
       " 'weston',\n",
       " 'whinny',\n",
       " 'wiener',\n",
       " 'wiesel',\n",
       " 'wigner',\n",
       " 'wilder',\n",
       " 'wilkes',\n",
       " 'wister',\n",
       " 'wooing',\n",
       " 'wright',\n",
       " 'yerkes',\n",
       " 'assign',\n",
       " 'befall',\n",
       " 'dampen',\n",
       " 'depute',\n",
       " 'detach',\n",
       " 'happen',\n",
       " 'quench',\n",
       " 'ablaze',\n",
       " 'aching',\n",
       " 'acidic',\n",
       " 'acinar',\n",
       " 'acinic',\n",
       " 'acting',\n",
       " 'affine',\n",
       " 'afghan',\n",
       " 'aflame',\n",
       " 'agamic',\n",
       " 'agonal',\n",
       " 'akimbo',\n",
       " 'amused',\n",
       " 'arabic',\n",
       " 'arable',\n",
       " 'ariled',\n",
       " 'ariose',\n",
       " 'arrant',\n",
       " 'askant',\n",
       " 'atrial',\n",
       " 'averse',\n",
       " 'baltic',\n",
       " 'banned',\n",
       " 'benign',\n",
       " 'better',\n",
       " 'bitter',\n",
       " 'bodied',\n",
       " 'bosomy',\n",
       " 'brawny',\n",
       " 'burned',\n",
       " 'center',\n",
       " 'cerise',\n",
       " 'cherry',\n",
       " 'closed',\n",
       " 'cloven',\n",
       " 'colour',\n",
       " 'crying',\n",
       " 'curtal',\n",
       " 'daedal',\n",
       " 'dickey',\n",
       " 'dilute',\n",
       " 'dipped',\n",
       " 'direct',\n",
       " 'dished',\n",
       " 'doomed',\n",
       " 'double',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'etched',\n",
       " 'evoked',\n",
       " 'exempt',\n",
       " 'fanned',\n",
       " 'fewest',\n",
       " 'fijian',\n",
       " 'filled',\n",
       " 'fitter',\n",
       " 'flying',\n",
       " 'foiled',\n",
       " 'forced',\n",
       " 'forged',\n",
       " 'formed',\n",
       " 'frozen',\n",
       " 'future',\n",
       " 'gabled',\n",
       " 'gallic',\n",
       " 'gelded',\n",
       " 'gentle',\n",
       " 'gifted',\n",
       " 'giving',\n",
       " 'glazed',\n",
       " 'grazed',\n",
       " 'hadean',\n",
       " 'hedged',\n",
       " 'heroic',\n",
       " 'hooked',\n",
       " 'hungry',\n",
       " 'hunted',\n",
       " 'hushed',\n",
       " 'hybrid',\n",
       " 'immune',\n",
       " 'ionian',\n",
       " 'jagged',\n",
       " 'jinxed',\n",
       " 'julian',\n",
       " 'korean',\n",
       " 'labile',\n",
       " 'legato',\n",
       " 'lesser',\n",
       " 'limber',\n",
       " 'linear',\n",
       " 'listed',\n",
       " 'little',\n",
       " 'loaded',\n",
       " 'madcap',\n",
       " 'master',\n",
       " 'mature',\n",
       " 'mellow',\n",
       " 'midway',\n",
       " 'mighty',\n",
       " 'minded',\n",
       " 'mobile',\n",
       " 'modern',\n",
       " 'molded',\n",
       " 'mosaic',\n",
       " 'motile',\n",
       " 'moving',\n",
       " 'naming',\n",
       " 'occult',\n",
       " 'otiose',\n",
       " 'pallid',\n",
       " 'patent',\n",
       " 'placid',\n",
       " 'played',\n",
       " 'polish',\n",
       " 'polite',\n",
       " 'poorly',\n",
       " 'potent',\n",
       " 'primed',\n",
       " 'proved',\n",
       " 'proven',\n",
       " 'raised',\n",
       " 'rancid',\n",
       " 'raring',\n",
       " 'rental',\n",
       " 'rheumy',\n",
       " 'ribbed',\n",
       " 'roiled',\n",
       " 'roofed',\n",
       " 'rubber',\n",
       " 'samoan',\n",
       " 'scanty',\n",
       " 'seated',\n",
       " 'senior',\n",
       " 'sexist',\n",
       " 'shaped',\n",
       " 'shared',\n",
       " 'signed',\n",
       " 'silver',\n",
       " 'simple',\n",
       " 'sinewy',\n",
       " 'siouan',\n",
       " 'sliced',\n",
       " 'slight',\n",
       " 'smooth',\n",
       " 'snooty',\n",
       " 'snotty',\n",
       " 'soaked',\n",
       " 'somali',\n",
       " 'sonsie',\n",
       " 'spoilt',\n",
       " 'spruce',\n",
       " 'square',\n",
       " 'squint',\n",
       " 'staged',\n",
       " 'stodgy',\n",
       " 'supple',\n",
       " 'syrian',\n",
       " 'taking',\n",
       " 'tapped',\n",
       " 'teased',\n",
       " 'tender',\n",
       " 'tested',\n",
       " 'tipped',\n",
       " 'tongan',\n",
       " 'topped',\n",
       " 'trying',\n",
       " 'tuscan',\n",
       " 'unable',\n",
       " 'uncool',\n",
       " 'unmown',\n",
       " 'untrue',\n",
       " 'uppish',\n",
       " 'valued',\n",
       " 'vested',\n",
       " 'virile',\n",
       " 'waning',\n",
       " 'wanton',\n",
       " 'washed',\n",
       " 'waxing',\n",
       " 'worthy',\n",
       " 'almost',\n",
       " 'forrad',\n",
       " 'longer',\n",
       " 'nearly',\n",
       " 'orally',\n",
       " 'plenty',\n",
       " 'thusly',\n",
       " 'withal',\n",
       " 'abatis',\n",
       " 'abseil',\n",
       " 'acetum',\n",
       " 'aclant',\n",
       " 'action',\n",
       " 'addict',\n",
       " 'admass',\n",
       " 'adrian',\n",
       " 'advert',\n",
       " 'alfred',\n",
       " 'allele',\n",
       " 'allies',\n",
       " 'amnion',\n",
       " 'amnios',\n",
       " 'amoxil',\n",
       " 'andrew',\n",
       " 'annwfn',\n",
       " 'answer',\n",
       " 'anthem',\n",
       " 'antony',\n",
       " 'apollo',\n",
       " 'appeal',\n",
       " 'araxes',\n",
       " 'ardour',\n",
       " 'argyle',\n",
       " 'argyll',\n",
       " 'artois',\n",
       " 'assist',\n",
       " 'athena',\n",
       " 'athene',\n",
       " 'athens',\n",
       " 'attila',\n",
       " 'aurora',\n",
       " 'auspex',\n",
       " 'axseed',\n",
       " 'baffle',\n",
       " 'balder',\n",
       " 'balzac',\n",
       " 'bandit',\n",
       " 'barite',\n",
       " 'barker',\n",
       " 'basics',\n",
       " 'basque',\n",
       " 'basuto',\n",
       " 'battle',\n",
       " 'beacon',\n",
       " 'beaker',\n",
       " 'becket',\n",
       " 'behalf',\n",
       " 'behmen',\n",
       " 'belloc',\n",
       " 'berber',\n",
       " 'bhakti',\n",
       " 'bikini',\n",
       " 'blenny',\n",
       " 'boards',\n",
       " 'bodoni',\n",
       " 'boehme',\n",
       " 'boleyn',\n",
       " 'boodle',\n",
       " 'borgia',\n",
       " 'bounty',\n",
       " 'bovril',\n",
       " 'braces',\n",
       " 'breeze',\n",
       " 'breton',\n",
       " 'brigid',\n",
       " 'bronte',\n",
       " 'browse',\n",
       " 'bruges',\n",
       " 'bubble',\n",
       " 'buddha',\n",
       " 'buffer',\n",
       " 'bunyan',\n",
       " 'burgoo',\n",
       " 'butler',\n",
       " 'butter',\n",
       " 'byrnie',\n",
       " 'cachou',\n",
       " 'caftan',\n",
       " 'callus',\n",
       " 'calpac',\n",
       " 'camera',\n",
       " 'canvas',\n",
       " 'carbon',\n",
       " 'carboy',\n",
       " 'carlos',\n",
       " 'carpet',\n",
       " 'catnap',\n",
       " 'cavell',\n",
       " 'cebuan',\n",
       " 'ceftin',\n",
       " 'censor',\n",
       " 'cercis',\n",
       " 'cervix',\n",
       " 'cesium',\n",
       " 'chadic',\n",
       " 'change',\n",
       " 'charge',\n",
       " 'cheese',\n",
       " 'chekov',\n",
       " 'chiron',\n",
       " 'cholla',\n",
       " 'christ',\n",
       " 'church',\n",
       " 'circus',\n",
       " 'cirrus',\n",
       " 'cleats',\n",
       " 'cloaca',\n",
       " 'clocks',\n",
       " 'clovis',\n",
       " 'cocain',\n",
       " 'colors',\n",
       " 'cooler',\n",
       " 'corbel',\n",
       " 'corner',\n",
       " 'coryza',\n",
       " 'cowage',\n",
       " 'crease',\n",
       " 'creeps',\n",
       " 'cronus',\n",
       " 'cuddle',\n",
       " 'curfew',\n",
       " 'curing',\n",
       " 'cursor',\n",
       " 'curtsy',\n",
       " 'cutlet',\n",
       " 'cutter',\n",
       " 'czerny',\n",
       " 'dalton',\n",
       " 'danaus',\n",
       " 'dangla',\n",
       " 'danube',\n",
       " 'darkey',\n",
       " 'darkie',\n",
       " 'darwin',\n",
       " 'dating',\n",
       " 'debate',\n",
       " 'decius',\n",
       " 'defile',\n",
       " 'degree',\n",
       " 'denali',\n",
       " 'denial',\n",
       " 'deputy',\n",
       " 'design',\n",
       " 'devise',\n",
       " 'diaper',\n",
       " 'dibbuk',\n",
       " 'dickie',\n",
       " 'diesel',\n",
       " 'digest',\n",
       " 'dining',\n",
       " 'divide',\n",
       " 'docket',\n",
       " 'doctor',\n",
       " 'doings',\n",
       " 'dollop',\n",
       " 'dongle',\n",
       " 'dorsum',\n",
       " 'douche',\n",
       " 'dragon',\n",
       " 'dredge',\n",
       " 'dybbuk',\n",
       " 'eating',\n",
       " 'ebitda',\n",
       " 'editor',\n",
       " 'egress',\n",
       " 'elavil',\n",
       " 'elijah',\n",
       " 'elixir',\n",
       " 'enbrel',\n",
       " 'encore',\n",
       " 'ensign',\n",
       " 'eolith',\n",
       " 'erebus',\n",
       " 'escape',\n",
       " 'esther',\n",
       " 'ethril',\n",
       " 'excuse',\n",
       " 'expose',\n",
       " 'eyelet',\n",
       " 'factor',\n",
       " 'fathom',\n",
       " 'fatima',\n",
       " 'faunus',\n",
       " 'fennic',\n",
       " 'fermat',\n",
       " 'ferret',\n",
       " 'finnic',\n",
       " 'firing',\n",
       " 'fizzle',\n",
       " 'florey',\n",
       " 'fodder',\n",
       " 'format',\n",
       " 'freeze',\n",
       " 'freyja',\n",
       " 'frigga',\n",
       " 'fright',\n",
       " 'frijol',\n",
       " 'frisch',\n",
       " 'galaxy',\n",
       " 'gallus',\n",
       " 'galois',\n",
       " 'gandhi',\n",
       " 'garter',\n",
       " 'geezer',\n",
       " 'george',\n",
       " 'giotto',\n",
       " 'godiva',\n",
       " 'gossip',\n",
       " 'grotto',\n",
       " 'growth',\n",
       " 'guyana',\n",
       " 'gypsum',\n",
       " 'haggle',\n",
       " 'hallah',\n",
       " 'hammer',\n",
       " 'harvey',\n",
       " 'hearts',\n",
       " 'hegari',\n",
       " 'helios',\n",
       " 'helper',\n",
       " 'herero',\n",
       " 'hermes',\n",
       " 'hestia',\n",
       " 'hobbes',\n",
       " 'hoenir',\n",
       " 'hootch',\n",
       " 'houdah',\n",
       " 'howdah',\n",
       " 'hudson',\n",
       " 'hulsea',\n",
       " 'humbug',\n",
       " 'humour',\n",
       " 'husain',\n",
       " 'husayn',\n",
       " 'huxley',\n",
       " 'hyades',\n",
       " 'hygeia',\n",
       " 'hypnos',\n",
       " 'icarus',\n",
       " 'infant',\n",
       " 'inocor',\n",
       " 'intake',\n",
       " 'iodine',\n",
       " 'ithunn',\n",
       " 'jaffar',\n",
       " 'jerome',\n",
       " 'joffre',\n",
       " 'jogger',\n",
       " 'joliot',\n",
       " 'joshua',\n",
       " 'kaftan',\n",
       " 'kalpac',\n",
       " 'kavrin',\n",
       " 'kazakh',\n",
       " 'kislev',\n",
       " 'kitbag',\n",
       " 'kittul',\n",
       " 'kleist',\n",
       " 'knight',\n",
       " 'laddie',\n",
       " 'larrea',\n",
       " 'lathee',\n",
       " 'lather',\n",
       " 'latium',\n",
       " 'latten',\n",
       " 'league',\n",
       " 'leakey',\n",
       " 'leging',\n",
       " 'legume',\n",
       " 'lipide',\n",
       " 'lipoid',\n",
       " 'lister',\n",
       " 'litmus',\n",
       " 'litter',\n",
       " 'looker',\n",
       " 'lorica',\n",
       " 'lounge',\n",
       " 'loyola',\n",
       " 'luther',\n",
       " 'lutzen',\n",
       " 'macron',\n",
       " 'madras',\n",
       " 'majors',\n",
       " 'making',\n",
       " 'manger',\n",
       " 'masher',\n",
       " 'masoud',\n",
       " 'masses',\n",
       " 'matron',\n",
       " 'mayhaw',\n",
       " 'maypop',\n",
       " 'medusa',\n",
       " 'melena',\n",
       " 'merckx',\n",
       " 'mihrab',\n",
       " 'mildew',\n",
       " 'milton',\n",
       " 'minyan',\n",
       " 'miosis',\n",
       " 'moppet',\n",
       " 'mosque',\n",
       " 'mother',\n",
       " 'motion',\n",
       " 'motown',\n",
       " 'mozart',\n",
       " 'mudcat',\n",
       " 'mugger',\n",
       " 'mumble',\n",
       " 'murmur',\n",
       " 'murray',\n",
       " 'mutter',\n",
       " 'muztag',\n",
       " 'myopia',\n",
       " 'myosis',\n",
       " 'nafcil',\n",
       " 'nagger',\n",
       " 'nectar',\n",
       " 'needle',\n",
       " 'nereus',\n",
       " 'newton',\n",
       " 'ningal',\n",
       " 'nissan',\n",
       " 'nivose',\n",
       " 'njorth',\n",
       " 'nodule',\n",
       " 'nolina',\n",
       " 'nootka',\n",
       " 'notice',\n",
       " 'notion',\n",
       " 'nowruz',\n",
       " 'nystan',\n",
       " 'office',\n",
       " 'oology',\n",
       " 'origen',\n",
       " 'pallas',\n",
       " 'pantie',\n",
       " 'parole',\n",
       " 'pascal',\n",
       " 'pashto',\n",
       " 'pashtu',\n",
       " 'patter',\n",
       " 'paynim',\n",
       " 'pepper',\n",
       " 'permit',\n",
       " 'perutz',\n",
       " 'pestle',\n",
       " 'petrol',\n",
       " 'phylum',\n",
       " 'pinion',\n",
       " 'pinter',\n",
       " 'plaint',\n",
       " 'plaque',\n",
       " 'plater',\n",
       " 'plavix',\n",
       " 'pledge',\n",
       " 'pliers']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_lesk(\"french speaking country \", 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bae20161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def noun_fn(sent,a,b):\n",
    "    text_words = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp_words = list(set(text_words) - set(stop_words))\n",
    "    if len(temp_words) != 0:\n",
    "        text_words = temp_words\n",
    "    syn = []\n",
    "    syn_noun = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_noun.extend(wn.synsets(val, pos=wn.NOUN))\n",
    "\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn.extend(wn.synsets(val))\n",
    "\n",
    "    output_lesk = {}\n",
    "    output_noun = {}\n",
    "    context_words = []\n",
    "\n",
    "    for i, val in enumerate(syn):\n",
    "        words = word_tokenize(val.definition())\n",
    "        context_words.extend(words)\n",
    "\n",
    "    context_words = list(set(context_words) - set(stop_words))\n",
    "\n",
    "    for word in list(wn.all_lemma_names(lang='eng')):\n",
    "        if (len(word)== a+b and word.isalpha()) or (len(word) == a+b+1 and (word[a] == '-' or word[a] == '_') and word[0:a].isalpha() and word[a+1:a+b+1].isalpha()):\n",
    "            sim_noun = 0\n",
    "            sim = 0\n",
    "            for synset in list(wn.synsets(word, pos=wn.NOUN)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_noun):\n",
    "                    temp = temp + synset.path_similarity(val)\n",
    "                sim_noun = max(temp, sim_noun)\n",
    "\n",
    "            for synset in list(wn.synsets(word)):\n",
    "                def_words = word_tokenize(synset.definition())\n",
    "                def_words = list(set(def_words) - set(stop_words))\n",
    "                temp1 = set(text_words).intersection(def_words)\n",
    "                temp2 = set(context_words).intersection(def_words)\n",
    "                temp1 = len(temp1) / len(text_words)\n",
    "                temp2 = len(temp2) / len(context_words)\n",
    "                t = 0.75 * temp1 + 0.25 * temp2\n",
    "                sim = max(t, sim)\n",
    "            if sim > 0:\n",
    "                output_lesk[word] = sim\n",
    "            if sim_noun > 0:\n",
    "                output_noun[word] = sim_noun\n",
    "    nouns = sorted(output_noun, key=output_noun.__getitem__, reverse=True)\n",
    "    lesk = sorted(output_lesk, key=output_lesk.__getitem__, reverse=True)\n",
    "\n",
    "    max_nouns = 0\n",
    "    for w in nouns:\n",
    "        if output_noun[w] > max_nouns:\n",
    "            max_nouns = output_noun[w]\n",
    "\n",
    "    for key, value in output_noun.items():\n",
    "        output_noun[key] = value / max_nouns\n",
    "\n",
    "    for w in lesk:\n",
    "        if not (w in output_noun):\n",
    "            output_noun[w] = 0\n",
    "        output_lesk[w] += output_noun[w]\n",
    "    rank = list(sorted(output_lesk, key=output_lesk.__getitem__, reverse=True))\n",
    "    final_rank=[]\n",
    "\n",
    "    for w in rank:\n",
    "        if w[a]=='_' or w[a]=='-':\n",
    "            p=w[:a]+w[a+1:]\n",
    "            final_rank.append(p)\n",
    "            rank.remove(w)\n",
    "        \n",
    "    rank1= final_rank + rank\n",
    "    return rank1[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef850b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def verb_fn(sent,a,b):\n",
    "    text_words = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp_words = list(set(text_words) - set(stop_words))\n",
    "    if len(temp_words) != 0:\n",
    "        text_words = temp_words\n",
    "    syn = []\n",
    "    syn_verb = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_verb.extend(wn.synsets(val, pos=wn.VERB))\n",
    "\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn.extend(wn.synsets(val))\n",
    "\n",
    "    output_lesk = {}\n",
    "    output_verb = {}\n",
    "    context_words = []\n",
    "\n",
    "    for i, val in enumerate(syn):\n",
    "        words = word_tokenize(val.definition())\n",
    "        context_words.extend(words)\n",
    "\n",
    "    context_words = list(set(context_words) - set(stop_words))\n",
    "\n",
    "    for word in list(wn.all_lemma_names(lang='eng')):\n",
    "        if (len(word)== a+b and word.isalpha()) or (len(word) == a+b+1 and (word[a] == '-' or word[a] == '_') and word[0:a].isalpha() and word[a+1:a+b+1].isalpha()):\n",
    "            sim_verb = 0\n",
    "            sim = 0\n",
    "            for synset in list(wn.synsets(word, pos=wn.VERB)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_verb):\n",
    "                    temp = temp + synset.path_similarity(val)\n",
    "                sim_verb = max(temp, sim_verb)\n",
    "\n",
    "            for synset in list(wn.synsets(word)):\n",
    "                def_words = word_tokenize(synset.definition())\n",
    "                def_words = list(set(def_words) - set(stop_words))\n",
    "                temp1 = set(text_words).intersection(def_words)\n",
    "                temp2 = set(context_words).intersection(def_words)\n",
    "                temp1 = len(temp1) / len(text_words)\n",
    "                temp2 = len(temp2) / len(context_words)\n",
    "                t = 0.75 * temp1 + 0.25 * temp2\n",
    "                sim = max(t, sim)\n",
    "            if sim > 0:\n",
    "                output_lesk[word] = sim\n",
    "            if sim_verb > 0:\n",
    "                output_verb[word] = sim_verb\n",
    "    verbs = sorted(output_verb, key=output_verb.__getitem__, reverse=True)\n",
    "    lesk = sorted(output_lesk, key=output_lesk.__getitem__, reverse=True)\n",
    "\n",
    "    max_verbs = 0\n",
    "    for w in verbs:\n",
    "        if output_verb[w] > max_verbs:\n",
    "            max_verbs = output_verb[w]\n",
    "\n",
    "    for key, value in output_verb.items():\n",
    "        output_verb[key] = value / max_verbs\n",
    "\n",
    "    for w in lesk:\n",
    "        if not (w in output_verb):\n",
    "            output_verb[w] = 0\n",
    "        output_lesk[w] += output_verb[w]\n",
    "    rank = list(sorted(output_lesk, key=output_lesk.__getitem__, reverse=True))\n",
    "\n",
    "    final_rank=[]\n",
    "\n",
    "    for w in rank:\n",
    "        if w[a]=='_' or w[a]=='-':\n",
    "            p=w[:a]+w[a+1:]\n",
    "            final_rank.append(p)\n",
    "            rank.remove(w)\n",
    "        \n",
    "    rank1= final_rank + rank\n",
    "    return rank1[:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2495ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adj_fn(sent,a,b):\n",
    "    text_words = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp_words = list(set(text_words) - set(stop_words))\n",
    "    if len(temp_words) != 0:\n",
    "        text_words = temp_words\n",
    "    syn = []\n",
    "    syn_adj = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_adj.extend(wn.synsets(val, pos=wn.ADJ))\n",
    "\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn.extend(wn.synsets(val))\n",
    "\n",
    "    output_lesk = {}\n",
    "    output_adj = {}\n",
    "    context_words = []\n",
    "\n",
    "    for i, val in enumerate(syn):\n",
    "        words = word_tokenize(val.definition())\n",
    "        context_words.extend(words)\n",
    "\n",
    "    context_words = list(set(context_words) - set(stop_words))\n",
    "\n",
    "    for word in list(wn.all_lemma_names(lang='eng')):\n",
    "        if (len(word)== a+b and word.isalpha()) or (len(word) == a+b+1 and (word[a] == '-' or word[a] == '_') and word[0:a].isalpha() and word[a+1:a+b+1].isalpha()):\n",
    "            sim_adj = 0\n",
    "            sim = 0\n",
    "            for synset in list(wn.synsets(word, pos=wn.ADJ)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_adj):\n",
    "                    if synset.path_similarity(val):\n",
    "                        temp = temp + synset.path_similarity(val)\n",
    "                sim_adj = max(temp, sim_adj)\n",
    "\n",
    "            for synset in list(wn.synsets(word)):\n",
    "                def_words = word_tokenize(synset.definition())\n",
    "                def_words = list(set(def_words) - set(stop_words))\n",
    "                temp1 = set(text_words).intersection(def_words)\n",
    "                temp2 = set(context_words).intersection(def_words)\n",
    "                temp1 = len(temp1) / len(text_words)\n",
    "                temp2 = len(temp2) / len(context_words)\n",
    "                t = 0.75 * temp1 + 0.25 * temp2\n",
    "                sim = max(t, sim)\n",
    "            if sim > 0:\n",
    "                output_lesk[word] = sim\n",
    "            if sim_adj > 0:\n",
    "                output_adj[word] = sim_adj\n",
    "    adjs = sorted(output_adj, key=output_adj.__getitem__, reverse=True)\n",
    "    lesk = sorted(output_lesk, key=output_lesk.__getitem__, reverse=True)\n",
    "\n",
    "    max_adjs = 0\n",
    "    for w in adjs:\n",
    "        if output_adj[w] > max_adjs:\n",
    "            max_adjs = output_adj[w]\n",
    "\n",
    "    for key, value in output_adj.items():\n",
    "        output_adj[key] = value / max_adjs\n",
    "\n",
    "    for w in lesk:\n",
    "        if not (w in output_adj):\n",
    "            output_adj[w] = 0\n",
    "        output_lesk[w] += output_adj[w]\n",
    "    rank = list(sorted(output_lesk, key=output_lesk.__getitem__, reverse=True))\n",
    "\n",
    "    final_rank=[]\n",
    "\n",
    "    for w in rank:\n",
    "        if w[a]=='_' or w[a]=='-':\n",
    "            p=w[:a]+w[a+1:];\n",
    "            final_rank.append(p);\n",
    "            rank.remove(w);\n",
    "        \n",
    "    rank1= final_rank + rank;\n",
    "    return rank1[:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5df4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_fn(sent,a,b):\n",
    "    text_words = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp_words = list(set(text_words) - set(stop_words))\n",
    "    if len(temp_words) != 0:\n",
    "        text_words = temp_words\n",
    "    syn = []\n",
    "    syn_adv = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_adv.extend(wn.synsets(val, pos=wn.ADV))\n",
    "\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn.extend(wn.synsets(val))\n",
    "\n",
    "    output_lesk = {}\n",
    "    output_adv = {}\n",
    "    context_words = []\n",
    "\n",
    "    for i, val in enumerate(syn):\n",
    "        words = word_tokenize(val.definition())\n",
    "        context_words.extend(words)\n",
    "\n",
    "    context_words = list(set(context_words) - set(stop_words))\n",
    "\n",
    "    for word in list(wn.all_lemma_names(lang='eng')):\n",
    "        if (len(word)== a+b and word.isalpha()) or (len(word) == a+b+1 and (word[a] == '-' or word[a] == '_') and word[0:a].isalpha() and word[a+1:a+b+1].isalpha()):\n",
    "            sim_adv = 0\n",
    "            sim = 0\n",
    "            for synset in list(wn.synsets(word, pos=wn.ADV)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_adv):\n",
    "                    if synset.path_similarity(val):\n",
    "                        temp = temp + synset.path_similarity(val)\n",
    "                sim_adv = max(temp, sim_adv)\n",
    "\n",
    "            for synset in list(wn.synsets(word)):\n",
    "                def_words = word_tokenize(synset.definition())\n",
    "                def_words = list(set(def_words) - set(stop_words))\n",
    "                temp1 = set(text_words).intersection(def_words)\n",
    "                temp2 = set(context_words).intersection(def_words)\n",
    "                temp1 = len(temp1) / len(text_words)\n",
    "                temp2 = len(temp2) / len(context_words)\n",
    "                t = 0.75 * temp1 + 0.25 * temp2\n",
    "                sim = max(t, sim)\n",
    "            if sim > 0:\n",
    "                output_lesk[word] = sim\n",
    "            if sim_adv > 0:\n",
    "                output_adv[word] = sim_adv\n",
    "    advs = sorted(output_adv, key=output_adv.__getitem__, reverse=True)\n",
    "    lesk = sorted(output_lesk, key=output_lesk.__getitem__, reverse=True)\n",
    "\n",
    "    max_advs = 0\n",
    "    for w in advs:\n",
    "        if output_adv[w] > max_advs:\n",
    "            max_advs = output_adv[w]\n",
    "\n",
    "    for key, value in output_adv.items():\n",
    "        output_adv[key] = value / max_advs\n",
    "\n",
    "    for w in lesk:\n",
    "        if not (w in output_adv):\n",
    "            output_adv[w] = 0\n",
    "        output_lesk[w] += output_adv[w]\n",
    "    rank = list(sorted(output_lesk, key=output_lesk.__getitem__, reverse=True))\n",
    "\n",
    "    final_rank=[]\n",
    "\n",
    "    for w in rank:\n",
    "        if w[a]=='_' or w[a]=='-':\n",
    "            p=w[:a]+w[a+1:]\n",
    "            final_rank.append(p)\n",
    "            rank.remove(w)\n",
    "        \n",
    "    rank1= final_rank + rank\n",
    "    return rank1[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b46d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dont_know_fn(sent, a, b):\n",
    "    \"\"\"\n",
    "    This function takes a sentence, a value 'a', and a value 'b' as input and performs word similarity calculations using WordNet.\n",
    "    It returns a ranked list of words based on their similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "    - sent (str): The input sentence.\n",
    "    - a (int): The first index for filtering words based on length and character conditions.\n",
    "    - b (int): The second index for filtering words based on length and character conditions.\n",
    "\n",
    "    Returns:\n",
    "    - rank1[:1000] (list): A ranked list of words based on their similarity scores, limited to the top 1000 words.\n",
    "    \"\"\"\n",
    "\n",
    "    text_words = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp_words = list(set(text_words) - set(stop_words))\n",
    "    if len(temp_words) != 0:\n",
    "        text_words = temp_words\n",
    "    syn = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn.extend(wn.synsets(val))\n",
    "\n",
    "    syn_verb = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_verb.extend(wn.synsets(val, pos=wn.VERB))\n",
    "\n",
    "    syn_noun = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_noun.extend(wn.synsets(val, pos=wn.NOUN))\n",
    "\n",
    "    syn_adj = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_adj.extend(wn.synsets(val, pos=wn.ADJ))\n",
    "\n",
    "    syn_adv = []\n",
    "    for i, val in enumerate(text_words):\n",
    "        syn_adv.extend(wn.synsets(val, pos=wn.ADV))\n",
    "\n",
    "\n",
    "    flag = 0\n",
    "    output_verb = {}\n",
    "    output_noun = {}\n",
    "    output_adj = {}\n",
    "    output_adv = {}\n",
    "    output_lesk = {}\n",
    "    context_words = []\n",
    "\n",
    "    for i, val in enumerate(syn):\n",
    "        words = word_tokenize(val.definition())\n",
    "        context_words.extend(words)\n",
    "\n",
    "    context_words = list(set(context_words) - set(stop_words))\n",
    "\n",
    "    for word in list(wn.all_lemma_names(lang='eng')):\n",
    "        if (len(word)== a+b and word.isalpha()) or (len(word) == a+b+1 and (word[a] == '-' or word[a] == '_') and word[0:a].isalpha() and word[a+1:a+b+1].isalpha()):\n",
    "            #print(\"hello\")\n",
    "            sim_verb = 0\n",
    "            sim_noun = 0\n",
    "            sim_adj = 0\n",
    "            sim_adv = 0\n",
    "            sim = 0\n",
    "            for synset in list(wn.synsets(word, pos=wn.VERB)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_verb):\n",
    "                    sim_verb = sim_verb + synset.path_similarity(val)\n",
    "                sim_verb = max(temp, sim_verb)\n",
    "            for synset in list(wn.synsets(word, pos=wn.NOUN)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_noun):\n",
    "                    temp = temp + synset.path_similarity(val)\n",
    "                sim_noun = max(temp, sim_noun)\n",
    "            for synset in list(wn.synsets(word, pos=wn.ADJ)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_adj):\n",
    "                    if synset.path_similarity(val):\n",
    "                        temp = temp + synset.path_similarity(val)\n",
    "                sim_adj = max(temp, sim_adj)\n",
    "            for synset in list(wn.synsets(word, pos=wn.ADV)):\n",
    "                temp = 0\n",
    "                for i, val in enumerate(syn_adv):\n",
    "                    if synset.path_similarity(val):\n",
    "                        temp = temp + synset.path_similarity(val)\n",
    "                sim_adv = max(temp, sim_adv)\n",
    "            for synset in list(wn.synsets(word)):\n",
    "                def_words = word_tokenize(synset.definition())\n",
    "                def_words = list(set(def_words) - set(stop_words))\n",
    "                temp1 = set(text_words).intersection(def_words)\n",
    "                temp2 = set(context_words).intersection(def_words)\n",
    "                temp1 = len(temp1) / len(text_words)\n",
    "                temp2 = len(temp2) / len(context_words)\n",
    "                t = 0.75 * temp1 + 0.25 * temp2\n",
    "                sim = max(t, sim)\n",
    "\n",
    "            if sim > 0:\n",
    "                output_lesk[word] = sim\n",
    "            if sim_verb > 0:\n",
    "                output_verb[word] = sim_verb\n",
    "            if sim_noun > 0:\n",
    "                output_noun[word] = sim_noun\n",
    "            if sim_adj > 0:\n",
    "                output_adj[word] = sim_adj\n",
    "            if sim_adv > 0:\n",
    "                output_adj[word] = sim_adv\n",
    "\n",
    "    verbs = sorted(output_verb, key=output_verb.__getitem__, reverse=True)\n",
    "    nouns = sorted(output_noun, key=output_noun.__getitem__, reverse=True)\n",
    "    adjs = sorted(output_adj, key=output_adj.__getitem__, reverse=True)\n",
    "    advs = sorted(output_adv, key=output_adv.__getitem__, reverse=True)\n",
    "    lesk = sorted(output_lesk, key=output_lesk.__getitem__, reverse=True)\n",
    "    max_verbs = 0\n",
    "    for w in verbs:\n",
    "        if output_verb[w] > max_verbs:\n",
    "            max_verbs = output_verb[w]\n",
    "\n",
    "    for key, value in output_verb.items():\n",
    "        output_verb[key] = value / max_verbs\n",
    "\n",
    "    max_nouns = 0\n",
    "    for w in nouns:\n",
    "        if output_noun[w] > max_nouns:\n",
    "            max_nouns = output_noun[w]\n",
    "\n",
    "    for key, value in output_noun.items():\n",
    "        output_noun[key] = value / max_nouns\n",
    "\n",
    "    max_adjs = 0\n",
    "    for w in adjs:\n",
    "        if output_adj[w] > max_adjs:\n",
    "            max_adjs = output_adj[w]\n",
    "\n",
    "    for key, value in output_adj.items():\n",
    "        output_adj[key] = value / max_adjs\n",
    "\n",
    "    max_advs = 0\n",
    "    for w in advs:\n",
    "        if output_adv[w] > max_advs:\n",
    "            max_advs = output_adv[w]\n",
    "\n",
    "    for key, value in output_adv.items():\n",
    "        output_adv[key] = value / max_advs\n",
    "\n",
    "    #tot_rank1 = dict(output_verb, **output_noun)\n",
    "    tot_rank1 = {k: max(i for i in (output_verb.get(k), output_noun.get(k)) if i) for k in\n",
    "                output_verb.keys() | output_noun}\n",
    "\n",
    "    #tot_rank = dict(tot_rank1, **output_adj)\n",
    "    tot_rank2 = {k: max(i for i in (tot_rank1.get(k), output_adj.get(k)) if i) for k in\n",
    "                tot_rank1.keys() | output_adj}\n",
    "\n",
    "    tot_rank = {k: max(i for i in (tot_rank2.get(k), output_adv.get(k)) if i) for k in\n",
    "                tot_rank2.keys() | output_adv}\n",
    "\n",
    "    for w in lesk:\n",
    "        if not (w in tot_rank):\n",
    "            tot_rank[w] = 0\n",
    "        tot_rank[w] += output_lesk[w]\n",
    "\n",
    "    rank = list(sorted(tot_rank, key=tot_rank.__getitem__, reverse=True))\n",
    "\n",
    "    final_rank=[]\n",
    "\n",
    "    for w in rank:\n",
    "        if w[a]=='_' or w[a]=='-':\n",
    "            p=w[:a]+w[a+1:]\n",
    "            final_rank.append(p)\n",
    "            rank.remove(w)\n",
    "        \n",
    "    rank1= final_rank + rank\n",
    "    \n",
    "    return rank1[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21abacb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856aeec4-d541-43df-98ba-076815b3a7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
